<!DOCTYPE html>
<html lang="cn" dir=>

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="术语  表 1: 术语  缩写 全称 含义    C Context 场景  CTR Click Through Rate 点击率  CVR Conversion Rate 转化率  I Item 物品  MLP Multilayer Perception 多层感知机  U User 用户     架构  图 1: 推荐系统架构  算法 不同算法的优缺点  表 2: 不同算法的优缺点  算法 优点 缺点    UserCF 符合直觉（兴趣相似的朋友喜欢的物品，我也喜欢） 用户数远大于物品数   社交特性更强，适于发现热点 用户历史数据向量很稀疏  ItemCF 适于兴趣变化较为稳定的应用 泛化能力弱，头部效应强   直观，可解释性强 无法有效引入场景信息  矩阵分解 泛化能力强 不方便融合特征   空间复杂度低 不好冷启动   便于与神经网络集成   逻辑回归 融合多种特征 无法特征交叉、筛选   假设 \(y\) 服从伯努利分布，有物理意义    是各特征的加权和，可解释性强    易于并行化、模型简单、易于训练   POLY2 特征交叉 特征向量更稀疏，不好训练    参数增多  FM 参数从 POLY2 的 \(n^2\) 下降到 \(nk\)    比 POLY2 更适于稀疏数据，泛化能力强    易于上线   FFM 比 FM 表达能力强 计算复杂度上升到 \(kn^2\)  GBDT&#43;LR 特征工程模型化   LS-PLM 能挖掘非线性模式    引入 L1 惩罚，模型稀疏   AutoRec 第一次使用深度学习框架   Deep Crossing 特征间深度交叉   NeuralCF 用户向量和物品向量更充分地交叉 没有引入更多特征   表达能力比矩阵分解强    可以灵活选择互操作层   PNN 强调不同特征之间的交互 简化操作丢失信息  Wide &amp; Deep 综合记忆能力和泛化能力    开拓了融合不同网络结构的新思路   Deep &amp; Cross       协同过滤 共现矩阵 用户为行坐标（记用户总数为 \(m\)）、物品为列坐标（即物品总数为 \(n\)）的 \(m\times n\) 维矩阵。">
<meta name="theme-color" content="#FFFFFF"><meta property="og:title" content="推荐系统" />
<meta property="og:description" content="术语  表 1: 术语  缩写 全称 含义    C Context 场景  CTR Click Through Rate 点击率  CVR Conversion Rate 转化率  I Item 物品  MLP Multilayer Perception 多层感知机  U User 用户     架构  图 1: 推荐系统架构  算法 不同算法的优缺点  表 2: 不同算法的优缺点  算法 优点 缺点    UserCF 符合直觉（兴趣相似的朋友喜欢的物品，我也喜欢） 用户数远大于物品数   社交特性更强，适于发现热点 用户历史数据向量很稀疏  ItemCF 适于兴趣变化较为稳定的应用 泛化能力弱，头部效应强   直观，可解释性强 无法有效引入场景信息  矩阵分解 泛化能力强 不方便融合特征   空间复杂度低 不好冷启动   便于与神经网络集成   逻辑回归 融合多种特征 无法特征交叉、筛选   假设 \(y\) 服从伯努利分布，有物理意义    是各特征的加权和，可解释性强    易于并行化、模型简单、易于训练   POLY2 特征交叉 特征向量更稀疏，不好训练    参数增多  FM 参数从 POLY2 的 \(n^2\) 下降到 \(nk\)    比 POLY2 更适于稀疏数据，泛化能力强    易于上线   FFM 比 FM 表达能力强 计算复杂度上升到 \(kn^2\)  GBDT&#43;LR 特征工程模型化   LS-PLM 能挖掘非线性模式    引入 L1 惩罚，模型稀疏   AutoRec 第一次使用深度学习框架   Deep Crossing 特征间深度交叉   NeuralCF 用户向量和物品向量更充分地交叉 没有引入更多特征   表达能力比矩阵分解强    可以灵活选择互操作层   PNN 强调不同特征之间的交互 简化操作丢失信息  Wide &amp; Deep 综合记忆能力和泛化能力    开拓了融合不同网络结构的新思路   Deep &amp; Cross       协同过滤 共现矩阵 用户为行坐标（记用户总数为 \(m\)）、物品为列坐标（即物品总数为 \(n\)）的 \(m\times n\) 维矩阵。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://kaizhang91.github.io/notes/docs/ai/recommend_system/" />
<meta property="article:modified_time" content="2020-10-13T07:19:32+00:00" />
<title>推荐系统 | 凯的笔记</title>
<link rel="manifest" href="/notes/manifest.json">
<link rel="icon" href="/notes/favicon.png" type="image/x-icon">
<link rel="stylesheet" href="/notes/book.min.3a7020b7bdb6727663c3b39ea050038a416a055141fadfe7e448887f123d96be.css" integrity="sha256-OnAgt722cnZjw7OeoFADikFqBVFB&#43;t/n5EiIfxI9lr4=">
<script defer src="/notes/cn.search.min.b9ff945956d2498560cb589418bd14d44bbaf4ebeebc5dac920779031e481f23.js" integrity="sha256-uf&#43;UWVbSSYVgy1iUGL0U1Eu69OvuvF2skgd5Ax5IHyM="></script>
<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
<link rel="icon" href="/notes/favicon.ico" type="image/x-icon">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dark.min.css">




</head>

<body dir=>
  <input type="checkbox" class="hidden toggle" id="menu-control" />
  <input type="checkbox" class="hidden toggle" id="toc-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/notes"><span>凯的笔记</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="搜索" aria-label="搜索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  <ul>
<li><strong>AI</strong>
<ul>
<li><a href="/notes/docs/ai/machine_learning/">机器学习</a></li>
<li><a href="/notes/docs/ai/dev_env/">开发环境</a></li>
<li><a href="/notes/docs/ai/recommend_system/"class=active>推荐系统</a></li>
<li><a href="/notes/docs/ai/nlp/">自然语言处理</a></li>
</ul>
</li>
<li><strong>数学</strong>
<ul>
<li><a href="/notes/docs/math/statistics/">统计</a></li>
<li><a href="/notes/docs/math/linear_algebra/">线性代数</a></li>
</ul>
</li>
<li><strong>视觉</strong>
<ul>
<li><a href="/notes/docs/visual/unity/">Unity</a></li>
<li><a href="/notes/docs/visual/video/">视频</a></li>
<li><a href="/notes/docs/visual/image/">图像</a></li>
</ul>
</li>
<li><strong>计算机科学</strong>
<ul>
<li><a href="/notes/docs/cs/http/">HTTP</a></li>
<li><a href="/notes/docs/cs/linux/">Linux</a></li>
<li><a href="/notes/docs/cs/oauth2/">OAuth 2</a></li>
<li><a href="/notes/docs/cs/security/">安全</a></li>
<li><a href="/notes/docs/cs/encoding/">编码</a></li>
<li><a href="/notes/docs/cs/ci/">持续集成</a></li>
<li><a href="/notes/docs/cs/transmission/">传输</a></li>
<li><a href="/notes/docs/cs/database/">数据库</a></li>
<li><a href="/notes/docs/cs/data-type/">数据类型</a></li>
<li><a href="/notes/docs/cs/algorithm/">算法</a></li>
<li><a href="/notes/docs/cs/network/">网络</a></li>
<li><a href="/notes/docs/cs/compress/">压缩</a></li>
</ul>
</li>
<li><strong>编程语言</strong>
<ul>
<li><a href="/notes/docs/program/bash/">Bash</a></li>
<li><a href="/notes/docs/program/css/">CSS</a></li>
<li><a href="/notes/docs/program/c++/">C++</a></li>
<li><a href="/notes/docs/program/dhall/">Dhall</a></li>
<li><a href="/notes/docs/program/fish-shell/">fish-shell</a></li>
<li><a href="/notes/docs/program/flutter/">Flutter</a></li>
<li><a href="/notes/docs/program/golang/">Golang</a></li>
<li><a href="/notes/docs/program/html/">HTML</a></li>
<li><a href="/notes/docs/program/js/">JavaScript</a></li>
<li><a href="/notes/docs/program/pandoc/">Pandoc</a></li>
<li><a href="/notes/docs/program/python/">Python</a></li>
<li><a href="/notes/docs/program/rust/">Rust</a></li>
<li><a href="/notes/docs/program/tex/">TeX</a></li>
<li><a href="/notes/docs/program/toml/">TOML</a></li>
<li><a href="/notes/docs/program/zsh/">Zsh</a></li>
<li><a href="/notes/docs/program/template/">模板</a></li>
</ul>
</li>
<li><strong>工具</strong>
<ul>
<li><a href="/notes/docs/tools/git/">Git</a></li>
<li><a href="/notes/docs/tools/linux/">Linux</a></li>
<li><a href="/notes/docs/tools/windows/">Windows</a></li>
<li><a href="/notes/docs/tools/editor/">编辑器</a></li>
<li><a href="/notes/docs/tools/format/">格式</a></li>
<li><a href="/notes/docs/tools/build/">构建</a></li>
<li><a href="/notes/docs/tools/browser/">浏览器</a></li>
<li><a href="/notes/docs/tools/search/">搜索</a></li>
<li><a href="/notes/docs/tools/reference-management/">文献管理</a></li>
<li><a href="/notes/docs/tools/project-management/">项目管理</a></li>
<li><a href="/notes/docs/tools/writing/">写作</a></li>
</ul>
</li>
</ul>










</nav>




  <script>!function(){var a=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(b){localStorage.setItem("menu.scrollTop",a.scrollTop)}),a.scrollTop=localStorage.getItem("menu.scrollTop")}()</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/notes/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>推荐系统</strong>

  <label for="toc-control">
    
    <img src="/notes/svg/toc.svg" class="book-icon" alt="Table of Contents" />
    
  </label>
</div>


  
  <aside class="hidden clearfix">
    
  


  </aside>
  
 
      </header>

      
      
  <article class="markdown"><h1 id="术语">术语</h1>
<div id="tbl:terminology">
<table>
<caption>表 1: 术语</caption>
<thead>
<tr class="header">
<th>缩写</th>
<th>全称</th>
<th>含义</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>C</td>
<td>Context</td>
<td>场景</td>
</tr>
<tr class="even">
<td>CTR</td>
<td>Click Through Rate</td>
<td>点击率</td>
</tr>
<tr class="odd">
<td>CVR</td>
<td>Conversion Rate</td>
<td>转化率</td>
</tr>
<tr class="even">
<td>I</td>
<td>Item</td>
<td>物品</td>
</tr>
<tr class="odd">
<td>MLP</td>
<td>Multilayer Perception</td>
<td>多层感知机</td>
</tr>
<tr class="even">
<td>U</td>
<td>User</td>
<td>用户</td>
</tr>
</tbody>
</table>
</div>
<h1 id="架构">架构</h1>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/architecture.webp" id="fig:architecture" alt="图 1: 推荐系统架构" /><figcaption aria-hidden="true">图 1: 推荐系统架构</figcaption>
</figure>
<h1 id="算法">算法</h1>
<h2 id="不同算法的优缺点">不同算法的优缺点</h2>
<div id="tbl:pros_cons">
<table>
<caption>表 2: 不同算法的优缺点</caption>
<thead>
<tr class="header">
<th>算法</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>UserCF</td>
<td>符合直觉（兴趣相似的朋友喜欢的物品，我也喜欢）</td>
<td>用户数远大于物品数</td>
</tr>
<tr class="even">
<td></td>
<td>社交特性更强，适于发现热点</td>
<td>用户历史数据向量很稀疏</td>
</tr>
<tr class="odd">
<td>ItemCF</td>
<td>适于兴趣变化较为稳定的应用</td>
<td>泛化能力弱，头部效应强</td>
</tr>
<tr class="even">
<td></td>
<td>直观，可解释性强</td>
<td>无法有效引入场景信息</td>
</tr>
<tr class="odd">
<td>矩阵分解</td>
<td>泛化能力强</td>
<td>不方便融合特征</td>
</tr>
<tr class="even">
<td></td>
<td>空间复杂度低</td>
<td>不好冷启动</td>
</tr>
<tr class="odd">
<td></td>
<td>便于与神经网络集成</td>
<td></td>
</tr>
<tr class="even">
<td>逻辑回归</td>
<td>融合多种特征</td>
<td>无法特征交叉、筛选</td>
</tr>
<tr class="odd">
<td></td>
<td>假设 <span class="math inline">\(y\)</span> 服从伯努利分布，有物理意义</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>是各特征的加权和，可解释性强</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>易于并行化、模型简单、易于训练</td>
<td></td>
</tr>
<tr class="even">
<td>POLY2</td>
<td>特征交叉</td>
<td>特征向量更稀疏，不好训练</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>参数增多</td>
</tr>
<tr class="even">
<td>FM</td>
<td>参数从 POLY2 的 <span class="math inline">\(n^2\)</span> 下降到 <span class="math inline">\(nk\)</span></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>比 POLY2 更适于稀疏数据，泛化能力强</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>易于上线</td>
<td></td>
</tr>
<tr class="odd">
<td>FFM</td>
<td>比 FM 表达能力强</td>
<td>计算复杂度上升到 <span class="math inline">\(kn^2\)</span></td>
</tr>
<tr class="even">
<td>GBDT+LR</td>
<td>特征工程模型化</td>
<td></td>
</tr>
<tr class="odd">
<td>LS-PLM</td>
<td>能挖掘非线性模式</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>引入 L1 惩罚，模型稀疏</td>
<td></td>
</tr>
<tr class="odd">
<td>AutoRec</td>
<td>第一次使用深度学习框架</td>
<td></td>
</tr>
<tr class="even">
<td>Deep Crossing</td>
<td>特征间深度交叉</td>
<td></td>
</tr>
<tr class="odd">
<td>NeuralCF</td>
<td>用户向量和物品向量更充分地交叉</td>
<td>没有引入更多特征</td>
</tr>
<tr class="even">
<td></td>
<td>表达能力比矩阵分解强</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>可以灵活选择互操作层</td>
<td></td>
</tr>
<tr class="even">
<td>PNN</td>
<td>强调不同特征之间的交互</td>
<td>简化操作丢失信息</td>
</tr>
<tr class="odd">
<td>Wide &amp; Deep</td>
<td>综合记忆能力和泛化能力</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>开拓了融合不同网络结构的新思路</td>
<td></td>
</tr>
<tr class="odd">
<td>Deep &amp; Cross</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h2 id="协同过滤">协同过滤</h2>
<h3 id="共现矩阵">共现矩阵</h3>
<p>用户为行坐标（记用户总数为 <span class="math inline">\(m\)</span>）、物品为列坐标（即物品总数为 <span class="math inline">\(n\)</span>）的 <span class="math inline">\(m\times n\)</span> 维矩阵。</p>
<h3 id="相似度">相似度</h3>
<h4 id="余弦相似度">余弦相似度</h4>
<p><span id="eq:cosine_similarity"><span class="math display">\[\sim(\v{i},\v{j}) = \cos(\v{i}, \v{j}) = \frac{\v{i}\cdot\v{j}}{\lVert\v{i}\rVert\cdot\lVert\v{j}\rVert}\qquad(1)\]</span></span></p>
<p>其中，<span class="math inline">\(\v{i}\)</span>、<span class="math inline">\(\v{j}\)</span> 均表示用户向量。</p>
<h4 id="皮尔逊相关系数">皮尔逊相关系数</h4>
<p><span id="eq:pearsion_coefficient"><span class="math display">\[\sim(i,j) = \frac{\sum_{p\in P}(R_{i,p} - \mean{R_i})(R_{j,p}-\mean{R_j})}{\sqrt{\sum_{p\in P}(R_{i,p}-\mean{R_i})^2}\sqrt{\sum_{p\in P}(R_{j,p}-\mean{R_j})^2}}\qquad(2)\]</span></span></p>
<p>其中，<span class="math inline">\(R_{i,p}\)</span> 表示用户 <span class="math inline">\(i\)</span> 对物品 <span class="math inline">\(p\)</span> 的评分；<span class="math inline">\(\mean{R_i}\)</span> 表示用户 <span class="math inline">\(i\)</span> 对所有物品的平均评分；<span class="math inline">\(P\)</span> 代表所有物品的集合。</p>
<blockquote>
<p>皮尔逊相关系数减小了用户评分偏置的影响。</p>
</blockquote>
<h4 id="皮尔逊相关系数拓展">皮尔逊相关系数拓展</h4>
<p><span id="eq:pearsion_coefficient_item"><span class="math display">\[\sim(i,j) = \frac{\sum_{p\in P}(R_{i,p} - \mean{R_p})(R_{j,p}-\mean{R_p})}{\sqrt{\sum_{p\in P}(R_{i,p}-\mean{R_p})^2}\sqrt{\sum_{p\in P}(R_{j,p}-\mean{R_p})^2}}\qquad(3)\]</span></span></p>
<p>其中，<span class="math inline">\(R_{i,p}\)</span> 表示用户 <span class="math inline">\(i\)</span> 对物品 <span class="math inline">\(p\)</span> 的评分；<span class="math inline">\(\mean{R_p}\)</span> 表示物品 <span class="math inline">\(p\)</span> 的平均评分；<span class="math inline">\(P\)</span> 代表所有物品的集合。</p>
<blockquote>
<p>式. 3 减小了物品评分偏置的影响。</p>
</blockquote>
<h3 id="usercf">UserCF</h3>
<p>基于用户的协同过滤。</p>
<p><span id="eq:user_cf"><span class="math display">\[R_{u,p} = \frac{\sum_{s\in S}w_{u,s}R_{s,p}}{\sum_{s\in S}w_{u,s}}\qquad(4)\]</span></span></p>
<p>其中，<span class="math inline">\(R_{s,p}\)</span> 表示用户 <span class="math inline">\(s\)</span> 对物品 <span class="math inline">\(p\)</span> 的评分；<span class="math inline">\(w_{u,s}\)</span> 表示用户 <span class="math inline">\(u\)</span> 与用户 <span class="math inline">\(s\)</span> 的相似度。</p>
<ul>
<li>基于用户的观看历史，找到跟目标用户看过同样视频的相似用户</li>
<li>找到这些相似用户喜欢看的其他视频</li>
</ul>
<h3 id="itemcf">ItemCF</h3>
<p>基于物品相似度的协同过滤。</p>
<p><span id="eq:item_cf"><span class="math display">\[R_{u,p} = \sum_{h\in H}w_{p,h}R_{u,h}\qquad(5)\]</span></span></p>
<p>其中，<span class="math inline">\(R_{u,h}\)</span> 表示用户 <span class="math inline">\(u\)</span> 对物品 <span class="math inline">\(h\)</span> 的评分；<span class="math inline">\(w_{p,h}\)</span> 表示物品 <span class="math inline">\(p\)</span> 与物品 <span class="math inline">\(h\)</span> 的相似度；<span class="math inline">\(H\)</span> 表示用户 <span class="math inline">\(u\)</span> 的正反馈物品集合。</p>
<h2 id="矩阵分解">矩阵分解</h2>
<p>分解共现矩阵得到用户和物品的隐向量：</p>
<p><span id="eq:matrix_factorization"><span class="math display">\[\m{R} = \m{U}\m{V}\qquad(6)\]</span></span></p>
<p>其中，<span class="math inline">\(\m{R}\)</span> 为 <span class="math inline">\(m\times n\)</span> 维的共现矩阵，<span class="math inline">\(\m{U}\)</span> 为 <span class="math inline">\(m\times k\)</span> 维的用户矩阵， <span class="math inline">\(\m{V}\)</span> 为 <span class="math inline">\(k\times n\)</span> 维的物品矩阵。</p>
<p><span id="eq:matrix_factorization_r"><span class="math display">\[\hat{r}_{ui} = \T{\v{q}}_i\v{p}_u\qquad(7)\]</span></span></p>
<p>其中，<span class="math inline">\(\v{p}_u\)</span> 表示 <span class="math inline">\(\m{U}\)</span> 的第 <span class="math inline">\(u\)</span> 行组成的向量，<span class="math inline">\(\v{q}_i\)</span> 表示 <span class="math inline">\(\m{V}\)</span> 中的第 <span class="math inline">\(i\)</span> 列组成的向量。</p>
<h3 id="奇异值分解">奇异值分解</h3>
<ul>
<li>共现矩阵有大量缺失值，不适于直接 SVD</li>
<li>计算复杂度高</li>
</ul>
<h3 id="梯度下降">梯度下降</h3>
<p>损失函数：</p>
<p><span id="eq:matrix_factorization_loss"><span class="math display">\[\min\sum_{(u,i)\in K}(r_{ui} - \T{\v{q}}_i\v{p}_u)^2\qquad(8)\]</span></span></p>
<p>其中，<span class="math inline">\(K\)</span> 是所有用户评分样本的集合。</p>
<h3 id="消除用户和物品打分的偏差">消除用户和物品打分的偏差</h3>
<p><span id="eq:matrix_factorization_r_bias"><span class="math display">\[\hat{r}_{ui} = \mu + b_i + b_u + \T{\v{q}}_i\v{p}_u\qquad(9)\]</span></span></p>
<p>其中，<span class="math inline">\(\mu\)</span> 是全局偏差，<span class="math inline">\(b_i\)</span> 是物品偏差，<span class="math inline">\(b_u\)</span> 是用户偏差。</p>
<h2 id="逻辑回归">逻辑回归</h2>
<h3 id="输出">输出</h3>
<p><span id="eq:lr_output"><span class="math display">\[\hat{y} = \sigmoid(\T{\v{x}}\v{w} + b)\qquad(10)\]</span></span></p>
<h3 id="损失函数">损失函数</h3>
<p><span id="eq:lr_loss"><span class="math display">\[
\begin{align}
J(\v{w}) &amp;= \frac{1}{m}\sum_{i=1}^m H(p(\v{y}), p(\hat{\v{y}})) \\
&amp;= -\frac{1}{m}\sum_{i=1}^m[y_i\log f_{\v{w}}(\v{x}_i) + (1-y_i)\log (1-f_{\v{w}}(\v{x}_i))]
\end{align}
\qquad(11)\]</span></span></p>
<blockquote>
<p>也可以用极大似然估计解释，<span class="math inline">\(P(y\mid \v{x};\v{w}) = (f_{\v{w}}(\v{x}))^y(1-f_{\v{w}}(\v{x}))^{1-y}\)</span>。</p>
</blockquote>
<h2 id="fm---ffm">FM -&gt; FFM</h2>
<h3 id="poly2">POLY2</h3>
<p><span id="eq:poly2"><span class="math display">\[\phi\mathrm{POLY2}(\v{w},\v{x}) =
\sum_{j_1=1}^{n-1}\sum_{j_2=j_1+1}^n w_{h(j_1,j_2)}x_{j_1}x_{j_2}\qquad(12)\]</span></span></p>
<h3 id="fm">FM</h3>
<p><span id="eq:fm"><span class="math display">\[\phi\mathrm{FM}(\v{w},\v{x}) =
\sum_{j_1=1}^{n-1}\sum_{j_2=j_1+1}^n (\v{w}_{j_1}\cdot\v{w}_{j_2})x_{j_1}x_{j_2}\qquad(13)\]</span></span></p>
<h3 id="ffm">FFM</h3>
<p><span id="eq:ffm"><span class="math display">\[\phi\mathrm{FFM}(\v{w},\v{x}) =
\sum_{j_1=1}^{n-1}\sum_{j_2=j_1+1}^n (\v{w}_{j_1,f_2}\cdot\v{w}_{j_2,f_1})x_{j_1}x_{j_2}\qquad(14)\]</span></span></p>
<h2 id="gbdt-lr">GBDT + LR</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/GBDT&#43;LR_architecture.jpg" id="fig:GBDT_LR_architecture" alt="图 2: GBDT+LR 架构" /><figcaption aria-hidden="true">图 2: GBDT+LR 架构</figcaption>
</figure>
<h2 id="ls-plm">LS-PLM</h2>
<h3 id="输出-1">输出</h3>
<p><span id="eq:ls_plm_y"><span class="math display">\[
\begin{align}
f(\v{x}) &amp;= \sum_{i=1}^m \pi_i(\v{x})\cdot\eta_i(\v{x}) \\
&amp;= \sum_{i=1}^m \frac{\e^{\v{\mu_i}\cdot\v{x}}}{\sum_{j=1}^m\e^{\v{\mu_j}\cdot\v{x}}}\cdot\frac{1}{1+\e^{-\v{w_i}\cdot\v{x}}}
\end{align}
\qquad(15)\]</span></span></p>
<p>其中，<span class="math inline">\(m\)</span> 为分片数，<span class="math inline">\(\pi\)</span> 为聚类函数（这里采用 softmax 对样本进行多分类）。</p>
<h2 id="autorec">AutoRec</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/auto_rec_architecture.png" id="fig:auto_rec_architecture" alt="图 3: AutoRec 架构[1]" /><figcaption aria-hidden="true">图 3: AutoRec 架构<span class="citation" data-cites="sedhain2015autorec"><sup>[<a href="#ref-sedhain2015autorec" role="doc-biblioref">1</a>]</sup></span></figcaption>
</figure>
<h3 id="损失函数-1">损失函数</h3>
<p><span id="eq:auto_rec_loss"><span class="math display">\[\min_{\theta}\left[\sum_{\v{r}\in S}\lVert\v{r}-h(\v{r};\theta)\rVert_2^2
+ \frac{\lambda}{2}(\lVert\m{W}_F^2\rVert + \lVert\m{V}\rVert_F^2)\right]\qquad(16)\]</span></span></p>
<p>其中，<span class="math inline">\(h(\v{r};\theta)\)</span> 为重建函数；<span class="math inline">\(\v{r}^{(i)}=\T{(R_{1i},\dots,R_{mi})}\)</span> 为物品 <span class="math inline">\(i\)</span> 的评分向量。</p>
<h3 id="重建函数">重建函数</h3>
<p><span id="eq:auto_rec_reconstruction"><span class="math display">\[h(\v{r};\theta)=f(\m{W}\cdot g(\m{V}\v{r}+\mu)+b)\qquad(17)\]</span></span></p>
<h3 id="输出-2">输出</h3>
<p><span id="eq:auto_rec_output"><span class="math display">\[\hat{R}_{ui} = h(\v{r}^{(i)};\theta)_u\qquad(18)\]</span></span></p>
<h2 id="deep-crossing">Deep Crossing</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/deep_crossing_architecture.png" id="fig:deep_crossing_architecture" alt="图 4: Deep Crossing 架构[2]" /><figcaption aria-hidden="true">图 4: Deep Crossing 架构<span class="citation" data-cites="shan2016deep"><sup>[<a href="#ref-shan2016deep" role="doc-biblioref">2</a>]</sup></span></figcaption>
</figure>
<h3 id="残差神经网络">残差神经网络</h3>
<p>好处：</p>
<ul>
<li>减少过拟合</li>
<li>减弱梯度小时现象，加快收敛速度</li>
</ul>
<h2 id="neuralcf">NeuralCF</h2>
<p>用多层神经网络代替矩阵分解的内积操作。</p>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/neural_cf_architecture.png" id="fig:neural_cf_architecture" alt="图 5: NeuralCF 架构[3]" /><figcaption aria-hidden="true">图 5: NeuralCF 架构<span class="citation" data-cites="he2017neural"><sup>[<a href="#ref-he2017neural" role="doc-biblioref">3</a>]</sup></span></figcaption>
</figure>
<h2 id="pnn">PNN</h2>
<p>乘积层代替 Deep Crossing 模型中的 Stacking 层。</p>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/pnn_architecture.png" id="fig:pnn_architecture" alt="图 6: PNN 架构[4]" /><figcaption aria-hidden="true">图 6: PNN 架构<span class="citation" data-cites="qu2016product"><sup>[<a href="#ref-qu2016product" role="doc-biblioref">4</a>]</sup></span></figcaption>
</figure>
<h2 id="wide-deep">Wide &amp; Deep</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/wide_deep_architecture.png" id="fig:wide_deep_architecture" alt="图 7: Wide &amp; Deep 架构[5]" /><figcaption aria-hidden="true">图 7: Wide &amp; Deep 架构<span class="citation" data-cites="cheng2016wide"><sup>[<a href="#ref-cheng2016wide" role="doc-biblioref">5</a>]</sup></span></figcaption>
</figure>
<p>Cross Product Transformation 为：</p>
<p><span class="math display">\[\phi_k(\v{x}) = \prod_{i=1}^d x_i^{c_{ki}}\quad c_{ki}\in\{0,1\}\]</span></p>
<p>其中，<span class="math inline">\(c_{ki}\)</span> 当第 <span class="math inline">\(i\)</span> 个特征属于第 <span class="math inline">\(k\)</span> 个组合特征时为 <span class="math inline">\(1\)</span>，否则为 <span class="math inline">\(0\)</span>；<span class="math inline">\(x_i\)</span> 是第 <span class="math inline">\(i\)</span> 个特征的值。</p>
<h2 id="deep-cross">Deep &amp; Cross</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/deep_cross_architecture.png" id="fig:deep_cross_architecture" style="width:60.0%" alt="图 8: Deep &amp; Cross 架构[6]" /><figcaption aria-hidden="true">图 8: Deep &amp; Cross 架构<span class="citation" data-cites="wang2017deep"><sup>[<a href="#ref-wang2017deep" role="doc-biblioref">6</a>]</sup></span></figcaption>
</figure>
<p>Cross 网络的运算为：</p>
<p><span id="eq:deep_cross_cross"><span class="math display">\[\v{x}_{l+1} = \v{x}_0\T{\v{x}_l}\v{w}_l + \v{b}_l + \v{x}_l\qquad(19)\]</span></span></p>
<h2 id="fnn">FNN</h2>
<p>用 FM 的隐向量初始化 Embedding 层。</p>
<p>为什么 Embedding 层收敛速度较慢？</p>
<ul>
<li>Embedding 参数多</li>
<li>输入向量稀疏</li>
</ul>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/fnn_architecture.png" id="fig:fnn_architecture" alt="图 9: FNN 架构[7]" /><figcaption aria-hidden="true">图 9: FNN 架构<span class="citation" data-cites="zhang2016deep"><sup>[<a href="#ref-zhang2016deep" role="doc-biblioref">7</a>]</sup></span></figcaption>
</figure>
<p>其中，<span class="math inline">\(\v{w}\)</span> 和 <span class="math inline">\(\v{v}\)</span> 对应于 FM 中的参数：</p>
<p><span id="eq:fm_output"><span class="math display">\[\hat{\v{y}} = \sigmoid\left(w_0 + \sum_{i=1}^N w_i x_i +
\sum_{i=1}^N\sum_{j=i+1}^N\langle\v{v}_i,\v{v}_j\rangle x_i x_j\right)
\qquad(20)\]</span></span></p>
<h2 id="deepfm">DeepFM</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/deep_fm_architecture.png" id="fig:deep_fm_architecture" alt="图 10: DeepFM 架构[8]" /><figcaption aria-hidden="true">图 10: DeepFM 架构<span class="citation" data-cites="guo2017deepfm"><sup>[<a href="#ref-guo2017deepfm" role="doc-biblioref">8</a>]</sup></span></figcaption>
</figure>
<h2 id="nfm">NFM</h2>
<p>用神经网络代替 FM 的二阶特征交叉。</p>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/nfm_architecture.png" id="fig:nfm_architecture" alt="图 11: NFM 架构[9]" /><figcaption aria-hidden="true">图 11: NFM 架构<span class="citation" data-cites="he2017nfm"><sup>[<a href="#ref-he2017nfm" role="doc-biblioref">9</a>]</sup></span></figcaption>
</figure>
<p>Bi-Interaction Pooling 具体操作为：</p>
<p><span id="eq:nfm_bi"><span class="math display">\[f_{\mathrm{BI}}(\mathcal{V}_x) = \sum_{i=1}^n\sum_{j=i+1}^n x_i\v{v}_i\odot
x_j\v{v}_j\qquad(21)\]</span></span></p>
<p>其中，<span class="math inline">\(\mathcal{V}_x\)</span> 是 Embedding 集合。</p>
<h2 id="afm">AFM</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/afm_architecture.png" id="fig:afm_architecture" alt="图 12: AFM 架构[10]" /><figcaption aria-hidden="true">图 12: AFM 架构<span class="citation" data-cites="xiao2017attentional"><sup>[<a href="#ref-xiao2017attentional" role="doc-biblioref">10</a>]</sup></span></figcaption>
</figure>
<p>特征交叉采用元素积：</p>
<p><span id="eq:afm_pi"><span class="math display">\[f_{\mathrm{PI}}(\mathcal{E}) = \{(\v{v}_i\odot\v{v}_j) x_i x_j\mid
(i,j)\in\mathcal{R}_x\}\qquad(22)\]</span></span></p>
<p>其中，PI 表示 Pair-wise Interaction Layer；<span class="math inline">\(\mathcal{E} = \{\v{v}_i x_i\mid i\in\mathcal{X}\}\)</span>；<span class="math inline">\(\mathcal{X}\)</span> 为非零特征的索引集合；<span class="math inline">\(\mathcal{R}_x = \{(i,j)\mid i\in\mathcal{X}, j\in\mathcal{X}, j&gt;i\}\)</span>。</p>
<p><span id="eq:afm_att"><span class="math display">\[
\begin{align}
f_{\mathrm{Att}}(f_{\mathrm{PI}}(\mathcal{E})) &amp;= \sum_{(i,j)\in\mathcal{R}_x}
a_{ij}(\v{v}_i\odot\v{v}_j) x_i x_j \\
a_{ij} &amp;= \frac{\exp(a&#39;_{ij})}{\sum_{(i,j)\in\mathcal{R}_x}\exp(a&#39;_{ij})} \\
a&#39;_{ij} &amp;= \T{\v{h}}\ReLU(\m{W}(\v{v}_i\odot\v{v}_j)x_i x_j +\v{b})
\end{align}
\qquad(23)\]</span></span></p>
<h2 id="din">DIN</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/din_architecture.png" id="fig:din_architecture" alt="图 13: DIN 架构[11]" /><figcaption aria-hidden="true">图 13: DIN 架构<span class="citation" data-cites="zhou2018deep"><sup>[<a href="#ref-zhou2018deep" role="doc-biblioref">11</a>]</sup></span></figcaption>
</figure>
<h3 id="注意力">注意力</h3>
<p><span id="eq:din_attention"><span class="math display">\[\v{v}_U(A) = f(\v{v}_A,\v{e}_1,\v{e}_2,\dots,\v{e}_H)
= \sum_{j=1}^H a(\v{e}_j,\v{v}_A)\v{e}_j
= \sum_{j=1}^H w_j\v{e}_j\qquad(24)\]</span></span></p>
<p>其中，<span class="math inline">\(\{\v{e}_1,\v{e}_2,\dots,\v{e}_H\}\)</span> 是用户 U 的历史行为的 Embedding 向量； <span class="math inline">\(\v{v}_A\)</span> 是广告 A 的 Embedding 向量。</p>
<h2 id="dien">DIEN</h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/recommend_system/dien_architecture.png" id="fig:dien_architecture" alt="图 14: DIEN 架构[12]" /><figcaption aria-hidden="true">图 14: DIEN 架构<span class="citation" data-cites="zhou2019deep"><sup>[<a href="#ref-zhou2019deep" role="doc-biblioref">12</a>]</sup></span></figcaption>
</figure>
<h3 id="augru">AUGRU</h3>
<p>AUGRU 改变了更新门：</p>
<p><span id="eq:dien_update_gate"><span class="math display">\[\tilde{\v{u}}&#39;_t = a_t\v{u}&#39;_t\qquad(25)\]</span></span></p>
<h1 class="unnumbered" id="参考文献">参考文献</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-sedhain2015autorec">
<p>[1] SEDHAIN S, MENON A K, SANNER S, 等. Autorec: Autoencoders meet collaborative filtering[C]//Proceedings of the 24th international conference on World Wide Web..</p>
</div>
<div id="ref-shan2016deep">
<p>[2] SHAN Y, HOENS T R, JIAO J, 等. Deep crossing: Web-scale modeling without manually crafted combinatorial features[C]//Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining..</p>
</div>
<div id="ref-he2017neural">
<p>[3] HE X, LIAO L, ZHANG H, 等. Neural collaborative filtering[C]//Proceedings of the 26th international conference on world wide web..</p>
</div>
<div id="ref-qu2016product">
<p>[4] QU Y, CAI H, REN K, 等. Product-based neural networks for user response prediction[C]//2016 IEEE 16th International Conference on Data Mining (ICDM). IEEE, 2016: 1149–1154.</p>
</div>
<div id="ref-cheng2016wide">
<p>[5] CHENG H-T, KOC L, HARMSEN J, 等. Wide &amp; deep learning for recommender systems[C]//Proceedings of the 1st workshop on deep learning for recommender systems..</p>
</div>
<div id="ref-wang2017deep">
<p>[6] WANG R, FU B, FU G, 等. Deep &amp; cross network for ad click predictions[M]//Proceedings of the ADKDD’17..</p>
</div>
<div id="ref-zhang2016deep">
<p>[7] ZHANG W, DU T, WANG J. Deep learning over multi-field categorical data[C]//European conference on information retrieval. Springer, 2016: 45–57.</p>
</div>
<div id="ref-guo2017deepfm">
<p>[8] GUO H, TANG R, YE Y, 等. DeepFM: a factorization-machine based neural network for CTR prediction[J]. arXiv preprint arXiv:1703.04247, 2017.</p>
</div>
<div id="ref-he2017nfm">
<p>[9] HE X, CHUA T-S. Neural factorization machines for sparse predictive analytics[C]//Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval..</p>
</div>
<div id="ref-xiao2017attentional">
<p>[10] XIAO J, YE H, HE X, 等. Attentional factorization machines: Learning the weight of feature interactions via attention networks[J]. arXiv preprint arXiv:1708.04617, 2017.</p>
</div>
<div id="ref-zhou2018deep">
<p>[11] ZHOU G, ZHU X, SONG C, 等. Deep interest network for click-through rate prediction[C]//Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining..</p>
</div>
<div id="ref-zhou2019deep">
<p>[12] ZHOU G, MOU N, FAN Y, 等. Deep interest evolution network for click-through rate prediction[C]//Proceedings of the AAAI conference on artificial intelligence..</p>
</div>
</div>

<nav id="TableOfContents" role="doc-toc">
  <h2 id="toc-title">目录</h2>
  <ul>
  <li><a href="#术语">术语</a></li>
  <li><a href="#架构">架构</a></li>
  <li><a href="#算法">算法</a>
  <ul>
  <li><a href="#不同算法的优缺点">不同算法的优缺点</a></li>
  <li><a href="#协同过滤">协同过滤</a>
  <ul>
  <li><a href="#共现矩阵">共现矩阵</a></li>
  <li><a href="#相似度">相似度</a></li>
  <li><a href="#usercf">UserCF</a></li>
  <li><a href="#itemcf">ItemCF</a></li>
  </ul></li>
  <li><a href="#矩阵分解">矩阵分解</a>
  <ul>
  <li><a href="#奇异值分解">奇异值分解</a></li>
  <li><a href="#梯度下降">梯度下降</a></li>
  <li><a href="#消除用户和物品打分的偏差">消除用户和物品打分的偏差</a></li>
  </ul></li>
  <li><a href="#逻辑回归">逻辑回归</a>
  <ul>
  <li><a href="#输出">输出</a></li>
  <li><a href="#损失函数">损失函数</a></li>
  </ul></li>
  <li><a href="#fm---ffm">FM -&gt; FFM</a>
  <ul>
  <li><a href="#poly2">POLY2</a></li>
  <li><a href="#fm">FM</a></li>
  <li><a href="#ffm">FFM</a></li>
  </ul></li>
  <li><a href="#gbdt-lr">GBDT + LR</a></li>
  <li><a href="#ls-plm">LS-PLM</a>
  <ul>
  <li><a href="#输出-1">输出</a></li>
  </ul></li>
  <li><a href="#autorec">AutoRec</a>
  <ul>
  <li><a href="#损失函数-1">损失函数</a></li>
  <li><a href="#重建函数">重建函数</a></li>
  <li><a href="#输出-2">输出</a></li>
  </ul></li>
  <li><a href="#deep-crossing">Deep Crossing</a>
  <ul>
  <li><a href="#残差神经网络">残差神经网络</a></li>
  </ul></li>
  <li><a href="#neuralcf">NeuralCF</a></li>
  <li><a href="#pnn">PNN</a></li>
  <li><a href="#wide-deep">Wide &amp; Deep</a></li>
  <li><a href="#deep-cross">Deep &amp; Cross</a></li>
  <li><a href="#fnn">FNN</a></li>
  <li><a href="#deepfm">DeepFM</a></li>
  <li><a href="#nfm">NFM</a></li>
  <li><a href="#afm">AFM</a></li>
  <li><a href="#din">DIN</a>
  <ul>
  <li><a href="#注意力">注意力</a></li>
  </ul></li>
  <li><a href="#dien">DIEN</a>
  <ul>
  <li><a href="#augru">AUGRU</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#参考文献">参考文献</a></li>
  </ul>
</nav>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex flex-wrap justify-between">



  <div><a class="flex align-center" href="https://github.com/kaizhang91/notes/commit/2e19f2061f474fc7ed2867aa40cdb7c2d6e8607e" title='最后修改者 Kai Zhang | 2020-10-13 15:19:32 &#43;0800' target="_blank" rel="noopener">
      <img src="/notes/svg/calendar.svg" class="book-icon" alt="Calendar" />
      <span>2020-10-13 15:19:32 &#43;0800</span>
    </a>
  </div>



</div>

 
        
      </footer>

      
  
  <div class="book-comments">

</div>
  
 

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      
  

 
    </aside>
    
  </main>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>

<script>
  
  const tableOfContents = document.getElementById("TableOfContents");
  const tableOfContents1 = tableOfContents.cloneNode(true);
  document.querySelector(".book-toc").appendChild(tableOfContents);
  document.querySelector(".book-header > aside").appendChild(tableOfContents1);

  
  const searchResults = document.getElementById("book-search-results");
  document
    .getElementById("book-search-input")
    .addEventListener("keyup", (event) => {
      
      
      if (event.keyCode === 13) {
        event.preventDefault();
        if (searchResults.childNodes.length > 0) {
          const searchResult = searchResults.childNodes[0].childNodes[0];
          window.location.href = searchResult.href;
        }
      }
    });

  
  MathJax = {
    tex: {
      macros: {
        AveP: ["\\mathrm{AveP}"],
        bm: ["{\\boldsymbol{#1}}", 1],
        cos: ["\\mathrm{cos}"],
        d: ["\\mathrm{d}"],
        data: ["\\mathrm{data}"],
        diag: ["\\mathrm{diag}"],
        e: ["\\mathrm{e}"],
        FN: ["\\mathrm{FN}"],
        FP: ["\\mathrm{FP}"],
        FPR: ["\\mathrm{FPR}"],
        log: ["\\mathrm{log}\\,"],
        MAP: ["\\mathrm{MAP}"],
        m: ["{\\bm{\\mathrm{#1}}}", 1],
        mean: ["{\\bar{#1}}", 1],
        median: ["\\mathrm{median}"],
        model: ["\\mathrm{model}"],
        R: ["\\mathbb{R}"],
        rel: ["\\mathrm{rel}"],
        ReLU: ["\\mathrm{ReLU}"],
        set: ["{\\mathbb{#1}}", 1],
        sigmoid: ["\\mathrm{sigmoid}"],
        sign: ["\\mathrm{sign}"],
        sim: ["\\mathrm{sim}"],
        SNR: ["\\mathrm{SNR}"],
        SO: ["\\mathrm{SO}"],
        tanh: ["\\mathrm{tanh}"],
        TN: ["\\mathrm{TN}"],
        TP: ["\\mathrm{TP}"],
        TPR: ["\\mathrm{TPR}"],
        transformation: ["{\\mathcal{#1}}", 1],
        T: ["{#1}^{\\mathsf{T}}", 1],
        v: ["{\\bm{\\mathrm{#1}}}", 1],
      },
    },
  };

  
  hljs.initHighlightingOnLoad();
  

</script>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>




</body>

</html>












