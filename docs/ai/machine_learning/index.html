<!DOCTYPE html>
<html lang="cn">

<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="机器学习"><meta property="og:title" content="机器学习" />
<meta property="og:description" content="中位数 评估指标  表 1: 混淆矩阵   实际 Positive 实际 Negative    预测 Positive True Positive False Positive（误报，Type I Error）  预测 Negative False Negative（漏报，Type II Error） True Negative     精确率（Precision） 你认为的正样本，有多少猜对了（猜的精确性如何）[1]：
\[ P = \frac{\TP}{\TP &#43; \FP}\qquad(1)\]
在信息检索领域这样定义[2]：
\[ P = \frac{\vert\{\textrm{relevant documents}\}\cap\{\textrm{retrieved documents}\}\vert}{\vert\{\textrm{retrieved documents}\}\vert}\qquad(2)\]
P@n 只考虑 top n 个查询结果。[2]
AveP(Average Precision)[2] \[\AveP = \int_0^1 p(r)\d r\qquad(3)\] \[\AveP = \sum_{k=1}^n P(k)\Delta r(k)\qquad(4)\] \[\AveP = \frac{\sum_{k=1}^n P(k)\times \rel(k)}{\vert\{\textrm{relevant documents}\}\vert}\qquad(5)\]" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://kaizhang91.github.io/notes/docs/ai/machine_learning/" />
<meta property="article:modified_time" content="2020-10-07T14:48:21+08:00" />
<title>机器学习 | 凯的笔记</title>
<link rel="icon" href="/notes/favicon.png" type="image/x-icon">


<link rel="stylesheet" href="/notes/book.min.8de2c8a2b5fa747b4ce910193be490c9875734ed2667d2b3bc3cace0afbb3f44.css" integrity="sha256-jeLIorX6dHtM6RAZO&#43;SQyYdXNO0mZ9KzvDys4K&#43;7P0Q=">


<script defer src="/notes/cn.search.min.dd129d29b8bcb2559abedb358c3329a9487fc617b5caa9dbfc74d528d250c52d.js" integrity="sha256-3RKdKbi8slWavts1jDMpqUh/xhe1yqnb/HTVKNJQxS0="></script>

<!--
Made with Book Theme
https://github.com/alex-shpak/hugo-book
-->

  
<link rel="icon" href="/notes/favicon.ico" type="image/x-icon">


<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dark.min.css">




</head>

<body>
  <input type="checkbox" class="hidden" id="menu-control" />
  <main class="container flex">
    <aside class="book-menu">
      
  <nav>
<h2 class="book-brand">
  <a href="/notes"><span>凯的笔记</span>
  </a>
</h2>


<div class="book-search">
  <input type="text" id="book-search-input" placeholder="搜索" aria-label="搜索" maxlength="64" data-hotkeys="s/" />
  <div class="book-search-spinner spinner hidden"></div>
  <ul id="book-search-results"></ul>
</div>











  <ul>
<li><strong>AI</strong>
<ul>
<li><a href="/notes/docs/ai/machine_learning/"class=active>机器学习</a></li>
<li><a href="/notes/docs/ai/dev_env/">开发环境</a></li>
<li><a href="/notes/docs/ai/recommend_system/">推荐系统</a></li>
<li><a href="/notes/docs/ai/nlp/">自然语言处理</a></li>
</ul>
</li>
<li><strong>数学</strong>
<ul>
<li><a href="/notes/docs/math/statistics/">统计</a></li>
<li><a href="/notes/docs/math/linear_algebra/">线性代数</a></li>
</ul>
</li>
<li><strong>视觉</strong>
<ul>
<li><a href="/notes/docs/visual/unity/">Unity</a></li>
<li><a href="/notes/docs/visual/video/">视频</a></li>
<li><a href="/notes/docs/visual/image/">图像</a></li>
</ul>
</li>
<li><strong>计算机科学</strong>
<ul>
<li><a href="/notes/docs/cs/http/">HTTP</a></li>
<li><a href="/notes/docs/cs/linux/">Linux</a></li>
<li><a href="/notes/docs/cs/oauth2/">OAuth 2</a></li>
<li><a href="/notes/docs/cs/security/">安全</a></li>
<li><a href="/notes/docs/cs/encoding/">编码</a></li>
<li><a href="/notes/docs/cs/ci/">持续集成</a></li>
<li><a href="/notes/docs/cs/transmission/">传输</a></li>
<li><a href="/notes/docs/cs/database/">数据库</a></li>
<li><a href="/notes/docs/cs/data-type/">数据类型</a></li>
<li><a href="/notes/docs/cs/algorithm/">算法</a></li>
<li><a href="/notes/docs/cs/network/">网络</a></li>
<li><a href="/notes/docs/cs/compress/">压缩</a></li>
</ul>
</li>
<li><strong>编程语言</strong>
<ul>
<li><a href="/notes/docs/program/bash/">Bash</a></li>
<li><a href="/notes/docs/program/css/">CSS</a></li>
<li><a href="/notes/docs/program/c++/">C++</a></li>
<li><a href="/notes/docs/program/dhall/">Dhall</a></li>
<li><a href="/notes/docs/program/fish-shell/">fish-shell</a></li>
<li><a href="/notes/docs/program/flutter/">Flutter</a></li>
<li><a href="/notes/docs/program/golang/">Golang</a></li>
<li><a href="/notes/docs/program/html/">HTML</a></li>
<li><a href="/notes/docs/program/js/">JavaScript</a></li>
<li><a href="/notes/docs/program/pandoc/">Pandoc</a></li>
<li><a href="/notes/docs/program/python/">Python</a></li>
<li><a href="/notes/docs/program/rust/">Rust</a></li>
<li><a href="/notes/docs/program/tex/">TeX</a></li>
<li><a href="/notes/docs/program/toml/">TOML</a></li>
<li><a href="/notes/docs/program/zsh/">Zsh</a></li>
<li><a href="/notes/docs/program/template/">模板</a></li>
</ul>
</li>
<li><strong>工具</strong>
<ul>
<li><a href="/notes/docs/tools/git/">Git</a></li>
<li><a href="/notes/docs/tools/linux/">Linux</a></li>
<li><a href="/notes/docs/tools/windows/">Windows</a></li>
<li><a href="/notes/docs/tools/editor/">编辑器</a></li>
<li><a href="/notes/docs/tools/format/">格式</a></li>
<li><a href="/notes/docs/tools/build/">构建</a></li>
<li><a href="/notes/docs/tools/browser/">浏览器</a></li>
<li><a href="/notes/docs/tools/search/">搜索</a></li>
<li><a href="/notes/docs/tools/reference-management/">文献管理</a></li>
<li><a href="/notes/docs/tools/project-management/">项目管理</a></li>
<li><a href="/notes/docs/tools/writing/">写作</a></li>
</ul>
</li>
</ul>










</nav>




  <script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script>


 
    </aside>

    <div class="book-page">
      <header class="book-header">
        
  <div class="flex align-center justify-between">
  <label for="menu-control">
    <img src="/notes/svg/menu.svg" class="book-icon" alt="Menu" />
  </label>

  <strong>机器学习</strong>

  <label for="toc-control">
    <img src="/notes/svg/toc.svg" class="book-icon" alt="Table of Contents" />
  </label>
</div>


  
    <input type="checkbox" class="hidden" id="toc-control" />
    <aside class="hidden clearfix">
      
  


    </aside>
  
 
      </header>

      
      
<article class="markdown"><h1 id="中位数">中位数</h1>
<h1 id="评估指标">评估指标</h1>
<div id="tbl:confusion_matrix">
<table>
<caption>表 1: 混淆矩阵</caption>
<thead>
<tr class="header">
<th></th>
<th>实际 Positive</th>
<th>实际 Negative</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>预测 Positive</td>
<td>True Positive</td>
<td>False Positive（误报，Type I Error）</td>
</tr>
<tr class="even">
<td>预测 Negative</td>
<td>False Negative（漏报，Type II Error）</td>
<td>True Negative</td>
</tr>
</tbody>
</table>
</div>
<h2 id="精确率precision">精确率（Precision）</h2>
<p>你认为的正样本，有多少猜对了（猜的精确性如何）<span class="citation" data-cites="ml_precision_recall"><sup>[<a href="#ref-ml_precision_recall" role="doc-biblioref">1</a>]</sup></span>：</p>
<p><span id="eq:precision"><span class="math display">\[ P = \frac{\TP}{\TP + \FP}\qquad(1)\]</span></span></p>
<p>在信息检索领域这样定义<span class="citation" data-cites="ml_evaluation_measures"><sup>[<a href="#ref-ml_evaluation_measures" role="doc-biblioref">2</a>]</sup></span>：</p>
<p><span id="eq:precision_information_retrieval"><span class="math display">\[ P = \frac{\vert\{\textrm{relevant documents}\}\cap\{\textrm{retrieved documents}\}\vert}{\vert\{\textrm{retrieved documents}\}\vert}\qquad(2)\]</span></span></p>
<h3 id="pn">P@n</h3>
<p>只考虑 top n 个查询结果。<span class="citation" data-cites="ml_evaluation_measures"><sup>[<a href="#ref-ml_evaluation_measures" role="doc-biblioref">2</a>]</sup></span></p>
<h3 id="avepaverage-precisionml_evaluation_measures">AveP(Average Precision)<span class="citation" data-cites="ml_evaluation_measures"><sup>[<a href="#ref-ml_evaluation_measures" role="doc-biblioref">2</a>]</sup></span></h3>
<p><span id="eq:AveP"><span class="math display">\[\AveP = \int_0^1 p(r)\d r\qquad(3)\]</span></span> <span id="eq:AveP_sum"><span class="math display">\[\AveP = \sum_{k=1}^n P(k)\Delta r(k)\qquad(4)\]</span></span> <span id="eq:AveP_k"><span class="math display">\[\AveP = \frac{\sum_{k=1}^n P(k)\times \rel(k)}{\vert\{\textrm{relevant documents}\}\vert}\qquad(5)\]</span></span></p>
<p>其中，</p>
<ul>
<li><span class="math inline">\(k\)</span> 表示在检索文档序列里的排名</li>
<li><span class="math inline">\(n\)</span> 表示检索文档的总数，<span class="math inline">\(P(k)\)</span> 表示 top k 的精确率</li>
<li><span class="math inline">\(\Delta r(k)\)</span> 表示从项 <span class="math inline">\(k-1\)</span> 到 <span class="math inline">\(k\)</span> 的召回率变化</li>
<li><span class="math inline">\(\rel(k)\)</span> 是指示函数，项 <span class="math inline">\(k\)</span> 是相关文档时为 <span class="math inline">\(1\)</span>，项 <span class="math inline">\(k\)</span> 不是相关文档时为 <span class="math inline">\(0\)</span></li>
</ul>
<h3 id="mapmean-average-precision">MAP(Mean Average Precision)</h3>
<p><span id="eq:map"><span class="math display">\[\MAP = \frac{\sum_{q=1}^Q \AveP(q)}{Q}\qquad(6)\]</span></span></p>
<p>其中 <span class="math inline">\(Q\)</span> 表示查询总数。</p>
<h2 id="召回率recall">召回率（Recall）</h2>
<p>正样本有多少被找出来了（召回了多少）<span class="citation" data-cites="ml_precision_recall"><sup>[<a href="#ref-ml_precision_recall" role="doc-biblioref">1</a>]</sup></span>：</p>
<p><span id="eq:recall"><span class="math display">\[R = \frac{\TP}{\TP + \FN}\qquad(7)\]</span></span></p>
<h2 id="准确率accuracy">准确率（Accuracy）</h2>
<p>预测正确的结果占总样本的比例<span class="citation" data-cites="ml_accuracy"><sup>[<a href="#ref-ml_accuracy" role="doc-biblioref">3</a>]</sup></span>：</p>
<p><span id="eq:accuracy"><span class="math display">\[A = \frac{\TP+\TN}{\TP+\TN+\FP+\FN}\qquad(8)\]</span></span></p>
<h2 id="f_1"><span class="math inline">\(F_1\)</span></h2>
<p>Precision 与 Recall 的调和均值：</p>
<p><span id="eq:f1"><span class="math display">\[\frac{2}{F_1} = \frac{1}{P} + \frac{1}{R}\qquad(9)\]</span></span></p>
<h2 id="真阳性率tprml_tpr_fpr">真阳性率（TPR）<span class="citation" data-cites="ml_tpr_fpr"><sup>[<a href="#ref-ml_tpr_fpr" role="doc-biblioref">4</a>]</sup></span></h2>
<p>在所有实际为阳性的样本中，被正确地判断为阳性之比率：</p>
<p><span id="eq:tpr"><span class="math display">\[ \TPR = \frac{\TP}{\TP + \FN}\qquad(10)\]</span></span></p>
<p>与召回率定义相同。</p>
<h2 id="伪阳性率fprml_tpr_fpr">伪阳性率（FPR）<span class="citation" data-cites="ml_tpr_fpr"><sup>[<a href="#ref-ml_tpr_fpr" role="doc-biblioref">4</a>]</sup></span></h2>
<p>在所有实际为阴性的样本中，被错误地判断为阳性之比率：</p>
<p><span id="eq:fpr"><span class="math display">\[ \FPR = \frac{\FP}{\FP + \TN}\qquad(11)\]</span></span></p>
<h2 id="roc-与-precision-recall-曲线ml_roc">ROC 与 Precision-Recall 曲线<span class="citation" data-cites="ml_roc"><sup>[<a href="#ref-ml_roc" role="doc-biblioref">5</a>]</sup></span></h2>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/machine_learning/roc_pr.png" id="fig:roc_pr" style="width:80.0%" alt="" /><figcaption>图 1: 一个 ROC 与 Precision-Recall 曲线例子</figcaption>
</figure>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/machine_learning/roc_pr.gif" id="fig:roc_pr_dynamic" style="width:80.0%" alt="" /><figcaption>图 2: ROC 与 Precision-Recall 曲线随阈值的动态变化</figcaption>
</figure>
<h3 id="roc-与-p-r-曲线的比较ml_roc">ROC 与 P-R 曲线的比较<span class="citation" data-cites="ml_roc"><sup>[<a href="#ref-ml_roc" role="doc-biblioref">5</a>]</sup></span></h3>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/machine_learning/roc_pr_imbalance.png" id="fig:roc_pr_imbalance" style="width:80.0%" alt="" /><figcaption>图 3: 样本不平衡时，ROC 与 Precision-Recall 曲线的比较</figcaption>
</figure>
<ul>
<li>当正负样本不平衡时，P-R 曲线对误报更敏感</li>
</ul>
<h3 id="aucarea-under-the-curve">AUC(Area Under the Curve)</h3>
<h4 id="roc-auc">ROC AUC</h4>
<figure>
<img src="https://kaizhang91.github.io/notes/fig/ai/machine_learning/roc_auc.svg" id="fig:roc_auc" style="width:80.0%" alt="" /><figcaption>图 4: ROC AUC 示意图</figcaption>
</figure>
<p>ROC AUC 表示随机正类别（绿色）样本位于随机负类别（红色）样本右侧的概率。<span class="citation" data-cites="ml_roc_auc"><sup>[<a href="#ref-ml_roc_auc" role="doc-biblioref">6</a>]</sup></span></p>
<h4 id="pr-auc">PR AUC</h4>
<p>PR AUC 表示在不同召回率阈值下的平均精确率。<span class="citation" data-cites="ml_pr_auc"><sup>[<a href="#ref-ml_pr_auc" role="doc-biblioref">7</a>]</sup></span></p>
<h1 id="信息论">信息论</h1>
<h2 id="条件熵conditional_entropy">条件熵<span class="citation" data-cites="conditional_entropy"><sup>[<a href="#ref-conditional_entropy" role="doc-biblioref">8</a>]</sup></span></h2>
<p><span id="eq:conditional_entropy"><span class="math display">\[
\begin{align}
H(Y\mid X) &amp;= \sum_{x\in\mathcal{X}}p(x)H(Y\mid X=x) \\
&amp;= -\sum_{x\in\mathcal{X},y\in\mathcal{Y}}p(x,y)\log\frac{p(x,y)}{p(x)}
\end{align}
\qquad(12)\]</span></span></p>
<h1 id="输出层">输出层</h1>
<h2 id="sigmoid">sigmoid</h2>
<p><span id="eq:sigmoid"><span class="math display">\[f(z) = \frac{1}{1+e^{-z}}\qquad(13)\]</span></span></p>
<h1 id="损失函数">损失函数</h1>
<h2 id="交叉熵cross_entropy">交叉熵<span class="citation" data-cites="cross_entropy"><sup>[<a href="#ref-cross_entropy" role="doc-biblioref">9</a>]</sup></span></h2>
<p><span id="eq:cross_entropy"><span class="math display">\[H(p, q) = -E_p[\log q]\qquad(14)\]</span></span></p>
<p>在离散情况下为</p>
<p><span id="eq:cross_entropy_discrete"><span class="math display">\[H(p, q) = -\sum_{x\in\mathcal{X}}p(x)\log q(x)\qquad(15)\]</span></span></p>
<h1 id="传统机器学习算法">传统机器学习算法</h1>
<h2 id="决策树">决策树</h2>
<ul>
<li>分类树：<span class="math inline">\(y\)</span> 离散</li>
<li>回归树：<span class="math inline">\(y\)</span> 连续</li>
</ul>
<h3 id="id3decision_tree">ID3<span class="citation" data-cites="decision_tree"><sup>[<a href="#ref-decision_tree" role="doc-biblioref">10</a>]</sup></span></h3>
<p>父节点的熵：</p>
<p><span class="math display">\[H(D) = -\sum_{k=1}^K\frac{|C_k|}{|D|}\log\frac{|C_k|}{|D|}\]</span></p>
<p>知道某个属性 A 后的条件熵：</p>
<p><span class="math display">\[H(D\mid A) = \sum_{i=1}^n\frac{|D_i|}{|D|}H(D_i)\]</span></p>
<p>信息增益：</p>
<p><span class="math display">\[\mathrm{Gain}(D, A) = H(D) - H(D\mid A)\]</span></p>
<p>缺点：</p>
<ul>
<li>信息增益偏向 <span class="math inline">\(|A|\)</span> 大的属性</li>
</ul>
<h3 id="c4.5">C4.5</h3>
<p>信息增益比：</p>
<p><span class="math display">\[\mathrm{Gain_{\mathrm{ratio}}}(D, A) = \frac{\mathrm{Gain}(D,A)}{H_A(D)}\]</span></p>
<p>其中，<span class="math inline">\(H_A(D) = -\sum_{i=1}^n\frac{|D_i|}{|D|}\log\frac{|D_i|}{|D|}\)</span>。</p>
<p>缺点：</p>
<ul>
<li>多叉树</li>
<li>只能分类</li>
</ul>
<h3 id="cart">CART</h3>
<h4 id="分类">分类</h4>
<p>基尼系数：</p>
<p><span class="math display">\[\mathrm{Gini}(D) = \sum_{k=1}^K\frac{|C_k|}{|D|}\left(1-\frac{|C_k|}{|D|}\right)\]</span> <span class="math display">\[\mathrm{Gini}(D, A) = \frac{|D_1|}{|D|}\mathrm{Gini}(D_1) + \frac{|D_2|}{|D|}\mathrm{Gini}(D_2)\]</span></p>
<p>用 <span class="math inline">\(\mathrm{Gini}(D, A)\)</span> 最小的属性 A 及切分点 a 分割 D。</p>
<h4 id="回归">回归</h4>
<p>按照</p>
<p><span class="math display">\[\min_{a, s}\left[\min_{c_1}\sum_{x_i\in D_1}(y_i - c_1)^2 + \min_{c_2}\sum_{x_i\in D_2}(y_i - c_2)^2\right]\]</span></p>
<p>选择属性 <span class="math inline">\(a\)</span> 与切分点 <span class="math inline">\(s\)</span>。</p>
<h2 id="随机森林">随机森林</h2>
<p>对 <span class="math inline">\(b=1,\dots,B\)</span>：</p>
<ul>
<li>从 <span class="math inline">\(X,Y\)</span> 进行 <span class="math inline">\(n\)</span> 次有放回的抽样，记为 <span class="math inline">\(X_b,Y_b\)</span></li>
<li>对 <span class="math inline">\(X_b,Y_b\)</span> 训练决策树 <span class="math inline">\(f_b\)</span>；每次分裂时只从特征的随机子集里选择</li>
<li><span class="math inline">\(\hat{f} = \frac{1}{B}\sum_{b=1}^B f_b(x)\)</span></li>
</ul>
<p>优点：</p>
<ul>
<li>泛化能力强</li>
<li>能处理高维数据，可以降维</li>
<li>可以处理缺失数据</li>
<li>可以并行</li>
</ul>
<p>缺点：</p>
<ul>
<li>黑盒</li>
<li>噪声较大时容易过拟合</li>
</ul>
<h2 id="gbdt">GBDT</h2>
<p>全称为 Gradient Boosting Decision Tree。</p>
<p>优点：</p>
<ul>
<li>比随机森林性能好</li>
<li>需要迭代训练，即串行计算</li>
</ul>
<h3 id="损失函数-1">损失函数</h3>
<p>Square loss:</p>
<p><span class="math display">\[L(y, F) = \frac{1}{2}(y-F(x))^2\]</span></p>
<p>Absolute loss:</p>
<p><span class="math display">\[L(y, F) = |y-F(x)|\]</span></p>
<p>Huber loss:</p>
<p><span class="math display">\[
L(y, F) =
\begin{cases}
\frac{1}{2}(y-F(x))^2, &amp; |y-F(x)| \le\delta\\
\delta(|y-F(x)| - \frac{\delta}{2}), &amp; |y-F(x)| &gt; \delta
\end{cases}
\]</span></p>
<div id="tbl:gbdt_loss_compare">
<table>
<caption>表 2: 不同损失函数的比较</caption>
<thead>
<tr class="header">
<th>损失函数</th>
<th>优点</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Square loss</td>
<td>数学性质好</td>
<td>对噪声敏感</td>
</tr>
<tr class="even">
<td>Absolute loss</td>
<td>对噪声鲁棒</td>
<td></td>
</tr>
<tr class="odd">
<td>Huber loss</td>
<td>对噪声更鲁棒</td>
<td></td>
</tr>
</tbody>
</table>
</div>
<h3 id="回归的学习步骤">回归的学习步骤</h3>
<ol type="1">
<li>初始化 <span class="math inline">\(F(x) = \frac{\sum_{i=1}^n y_i}{n}\)</span></li>
<li>计算负梯度：<span class="math inline">\(-g(x_i) = -\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}\)</span></li>
<li>按 <span class="math inline">\(-g(x_i)\)</span> 拟合回归树 <span class="math inline">\(h\)</span></li>
<li>更新 F，<span class="math inline">\(F := F + \rho h\)</span></li>
<li>终止或者跳到步骤 2</li>
</ol>
<h2 id="xgboost">XGBoost</h2>
<h3 id="树的输出">树的输出</h3>
<p><span class="math display">\[f^{(t)}(\v{x}) = w_{q(\v{x})}, \v{w}\in \R^T, q: \R^d\to \{1,2,\cdots,T\}\]</span></p>
<p>其中，<span class="math inline">\(T\)</span> 为叶子节点的数量；<span class="math inline">\(d\)</span> 为 <span class="math inline">\(\v{x}\)</span> 的维度；<span class="math inline">\(q\)</span> 把 <span class="math inline">\(\v{x}\)</span> 映射到某个叶子序号。</p>
<p><span class="math display">\[w_j = -\frac{G_j}{H_j + \lambda}\]</span> <span class="math display">\[G_j = \sum_{i\in I_j}g_i\]</span> <span class="math display">\[H_j = \sum_{i\in I_j}h_i\]</span> <span class="math display">\[I_j = \{i\mid q(\v{x}_i) = j\}\]</span> <span class="math display">\[g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial\hat{y}_i^{(t-1)}}\]</span> <span class="math display">\[h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{{\partial\hat{y}_i^{(t-1)}}^2}\]</span></p>
<h3 id="树的损失函数">树的损失函数</h3>
<p><span class="math display">\[J(f^{(t)}) = -\frac{1}{2}\sum_{j=1}^T\frac{G_j^2}{H_j+\lambda} + \gamma T\]</span></p>
<h3 id="节点分裂的增益">节点分裂的增益</h3>
<p><span class="math display">\[\mathrm{Gain} = \frac{1}{2}\left[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right] - \gamma\]</span></p>
<p>优点：</p>
<ul>
<li>比 GBDT 性能更好</li>
<li>并行优化</li>
</ul>
<p>与 GBDT 的区别：</p>
<ul>
<li>除了支持 CART 树，还支持线性分类器</li>
<li>加了二阶导数</li>
<li>加了正则项，防止过拟合</li>
<li>支持列抽样</li>
<li>可以自动学习缺失值的分裂方向</li>
<li>预先排序，并行计算不同特征的增益</li>
</ul>
<h2 id="lightgbm">LightGBM</h2>
<p>与 XGBoost 的区别：</p>
<ul>
<li>XGBoost 是 pre-sorted 算法，寻找数据分割点更精确；LightGBM 是 histogram 算法，占用内存小，计算复杂度低</li>
<li>XGBoost 是 level-wise 分裂，LightGBM 是 leaf-wise</li>
<li>XGBoost 使用 pre-sorted 算法，通信代价大；LightGBM 使用 histogram 算法，通信代价小，能并行计算</li>
</ul>
<h1 class="unnumbered" id="参考文献" class="unnumbered">参考文献</h1>
<div id="refs" class="references" role="doc-bibliography">
<div id="ref-ml_precision_recall">
<p>[1] 张俊博. 如何解释召回率与精确率？[EB/OL](2018–05–08). <a href="https://www.zhihu.com/question/19645541/answer/360749257">https://www.zhihu.com/question/19645541/answer/360749257</a>.</p>
</div>
<div id="ref-ml_evaluation_measures">
<p>[2] WIKIPEDIA. Evaluation measures (information retrieval)[EB/OL]([日期不详]). <a href="https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)">https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)</a>.</p>
</div>
<div id="ref-ml_accuracy">
<p>[3] EASYAI. 一文看懂机器学习指标：准确率、精准率、召回率、F1、ROC曲线、AUC曲线[EB/OL](2019–11–21). <a href="https://medium.com/@pkqiang49/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8C%87%E6%A0%87-%E5%87%86%E7%A1%AE%E7%8E%87-%E7%B2%BE%E5%87%86%E7%8E%87-%E5%8F%AC%E5%9B%9E%E7%8E%87-f1-roc%E6%9B%B2%E7%BA%BF-auc%E6%9B%B2%E7%BA%BF-19b1ed9e9260">https://medium.com/@pkqiang49/%E4%B8%80%E6%96%87%E7%9C%8B%E6%87%82%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%8C%87%E6%A0%87-%E5%87%86%E7%A1%AE%E7%8E%87-%E7%B2%BE%E5%87%86%E7%8E%87-%E5%8F%AC%E5%9B%9E%E7%8E%87-f1-roc%E6%9B%B2%E7%BA%BF-auc%E6%9B%B2%E7%BA%BF-19b1ed9e9260</a>.</p>
</div>
<div id="ref-ml_tpr_fpr">
<p>[4] 维基百科. ROC 曲线[EB/OL]([日期不详]). <a href="https://zh.wikipedia.org/zh-cn/ROC%E6%9B%B2%E7%BA%BF">https://zh.wikipedia.org/zh-cn/ROC%E6%9B%B2%E7%BA%BF</a>.</p>
</div>
<div id="ref-ml_roc">
<p>[5] AZEVEDO C. On ROC and Precision-Recall curves[EB/OL]([日期不详]). <a href="https://towardsdatascience.com/on-roc-and-precision-recall-curves-c23e9b63820c">https://towardsdatascience.com/on-roc-and-precision-recall-curves-c23e9b63820c</a>.</p>
</div>
<div id="ref-ml_roc_auc">
<p>[6] GOOGLE. 分类 (Classification)：ROC 和曲线下面积[EB/OL]([日期不详]). <a href="https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=zh-cn">https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=zh-cn</a>.</p>
</div>
<div id="ref-ml_pr_auc">
<p>[7] NEPTUNE.AI. F1 Score vs ROC AUC vs Accuracy vs PR AUC: Which Evaluation Metric Should You Choose?[EB/OL](2019–11–04). <a href="https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc#3">https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc#3</a>.</p>
</div>
<div id="ref-conditional_entropy">
<p>[8] WIKIPEDIA. Conditional entropy[EB/OL]([日期不详]). <a href="https://en.wikipedia.org/wiki/Conditional_entropy">https://en.wikipedia.org/wiki/Conditional_entropy</a>.</p>
</div>
<div id="ref-cross_entropy">
<p>[9] WIKIPEDIA. Cross entropy[EB/OL]([日期不详]). <a href="https://en.wikipedia.org/wiki/Cross_entropy">https://en.wikipedia.org/wiki/Cross_entropy</a>.</p>
</div>
<div id="ref-decision_tree">
<p>[10] 阿泽. 【机器学习】决策树（上）——ID3、C4.5、CART（非常详细）[EB/OL](2019–10–09). <a href="https://zhuanlan.zhihu.com/p/85731206">https://zhuanlan.zhihu.com/p/85731206</a>.</p>
</div>
</div>

<nav id="TableOfContents" role="doc-toc">
  <h2 id="toc-title">目录</h2>
  <ul>
  <li><a href="#中位数">中位数</a></li>
  <li><a href="#评估指标">评估指标</a>
  <ul>
  <li><a href="#精确率precision">精确率（Precision）</a>
  <ul>
  <li><a href="#pn">P@n</a></li>
  <li><a href="#avepaverage-precisionml_evaluation_measures">AveP(Average Precision)<span class="citation" data-cites="ml_evaluation_measures"><sup>[<span>2</span>]</sup></span></a></li>
  <li><a href="#mapmean-average-precision">MAP(Mean Average Precision)</a></li>
  </ul></li>
  <li><a href="#召回率recall">召回率（Recall）</a></li>
  <li><a href="#准确率accuracy">准确率（Accuracy）</a></li>
  <li><a href="#f_1"><span class="math inline">\(F_1\)</span></a></li>
  <li><a href="#真阳性率tprml_tpr_fpr">真阳性率（TPR）<span class="citation" data-cites="ml_tpr_fpr"><sup>[<span>4</span>]</sup></span></a></li>
  <li><a href="#伪阳性率fprml_tpr_fpr">伪阳性率（FPR）<span class="citation" data-cites="ml_tpr_fpr"><sup>[<span>4</span>]</sup></span></a></li>
  <li><a href="#roc-与-precision-recall-曲线ml_roc">ROC 与 Precision-Recall 曲线<span class="citation" data-cites="ml_roc"><sup>[<span>5</span>]</sup></span></a>
  <ul>
  <li><a href="#roc-与-p-r-曲线的比较ml_roc">ROC 与 P-R 曲线的比较<span class="citation" data-cites="ml_roc"><sup>[<span>5</span>]</sup></span></a></li>
  <li><a href="#aucarea-under-the-curve">AUC(Area Under the Curve)</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#信息论">信息论</a>
  <ul>
  <li><a href="#条件熵conditional_entropy">条件熵<span class="citation" data-cites="conditional_entropy"><sup>[<span>8</span>]</sup></span></a></li>
  </ul></li>
  <li><a href="#输出层">输出层</a>
  <ul>
  <li><a href="#sigmoid">sigmoid</a></li>
  </ul></li>
  <li><a href="#损失函数">损失函数</a>
  <ul>
  <li><a href="#交叉熵cross_entropy">交叉熵<span class="citation" data-cites="cross_entropy"><sup>[<span>9</span>]</sup></span></a></li>
  </ul></li>
  <li><a href="#传统机器学习算法">传统机器学习算法</a>
  <ul>
  <li><a href="#决策树">决策树</a>
  <ul>
  <li><a href="#id3decision_tree">ID3<span class="citation" data-cites="decision_tree"><sup>[<span>10</span>]</sup></span></a></li>
  <li><a href="#c4.5">C4.5</a></li>
  <li><a href="#cart">CART</a></li>
  </ul></li>
  <li><a href="#随机森林">随机森林</a></li>
  <li><a href="#gbdt">GBDT</a>
  <ul>
  <li><a href="#损失函数-1">损失函数</a></li>
  <li><a href="#回归的学习步骤">回归的学习步骤</a></li>
  </ul></li>
  <li><a href="#xgboost">XGBoost</a>
  <ul>
  <li><a href="#树的输出">树的输出</a></li>
  <li><a href="#树的损失函数">树的损失函数</a></li>
  <li><a href="#节点分裂的增益">节点分裂的增益</a></li>
  </ul></li>
  <li><a href="#lightgbm">LightGBM</a></li>
  </ul></li>
  <li><a href="#参考文献">参考文献</a></li>
  </ul>
</nav>
</article>
 
      

      <footer class="book-footer">
        
  <div class="flex justify-between">



  <div>
    
    <a class="flex align-center" href="https://github.com/kaizhang91/notes/commit/af9a1a1338c83ed3f42d19578d1faacc26866d82" title='最后修改者 Kai Zhang | 2020-10-07 14:48:21 &#43;0800' target="_blank" rel="noopener">
      <img src="/notes/svg/calendar.svg" class="book-icon" alt="Calendar" />
      <span>2020-10-07 14:48:21 &#43;0800</span>
    </a>
  </div>



</div>

 
        
  
  <div class="book-comments">

</div>
  
 
      </footer>
      

      <label for="menu-control" class="hidden book-menu-overlay"></label>
    </div>

    
    <aside class="book-toc">
      
  

 
    </aside>
    
  </main>

  
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js"></script>

<script>
  
  const tableOfContents = document.getElementById("TableOfContents");
  const tableOfContents1 = tableOfContents.cloneNode(true);
  document.querySelector(".book-toc").appendChild(tableOfContents);
  document
    .getElementById("toc-control")
    .nextElementSibling.appendChild(tableOfContents1);

  
  const searchResults = document.getElementById("book-search-results");
  document
    .getElementById("book-search-input")
    .addEventListener("keyup", (event) => {
      
      
      if (event.keyCode === 13) {
        event.preventDefault();
        if (searchResults.childNodes.length > 0) {
          const searchResult = searchResults.childNodes[0].childNodes[0];
          window.location.href = searchResult.href;
        }
      }
    });

  
  MathJax = {
    tex: {
      macros: {
        AveP: ["\\mathrm{AveP}"],
        bm: ["{\\boldsymbol{#1}}", 1],
        cos: ["\\mathrm{cos}"],
        d: ["\\mathrm{d}"],
        data: ["\\mathrm{data}"],
        diag: ["\\mathrm{diag}"],
        FN: ["\\mathrm{FN}"],
        FP: ["\\mathrm{FP}"],
        FPR: ["\\mathrm{FPR}"],
        log: ["\\mathrm{log}\\,"],
        MAP: ["\\mathrm{MAP}"],
        m: ["{\\bm{#1}}", 1],
        mean: ["{\\bar{#1}}", 1],
        median: ["\\mathrm{median}"],
        model: ["\\mathrm{model}"],
        R: ["\\mathbb{R}"],
        rel: ["\\mathrm{rel}"],
        set: ["{\\mathbb{#1}}", 1],
        sigmoid: ["\\mathrm{sigmoid}"],
        sign: ["\\mathrm{sign}"],
        sim: ["\\mathrm{sim}"],
        SNR: ["\\mathrm{SNR}"],
        SO: ["\\mathrm{SO}"],
        tanh: ["\\mathrm{tanh}"],
        TN: ["\\mathrm{TN}"],
        TP: ["\\mathrm{TP}"],
        TPR: ["\\mathrm{TPR}"],
        transformation: ["{\\mathcal{#1}}", 1],
        T: ["{#1}^{\\mathsf{T}}", 1],
        v: ["{\\bm{#1}}", 1],
      },
    },
  };

  
  hljs.initHighlightingOnLoad();
  


  
  window.setTimeout(() => {
    document.querySelectorAll("table").forEach((table) => {
      
      
      
      const tableWidth = table.caption.clientWidth + 1;
      table.style.width = tableWidth + "px";
    });
  }, 100);
</script>


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>




</body>

</html>












